{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'Blues'\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statsmodels\n",
    "import json\n",
    "import math\n",
    "import imageio\n",
    "import itertools\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import clear_output\n",
    "from functools import reduce, partial\n",
    "from scipy.stats import linregress, t, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>\n",
    "# Running Synthesis Validation\n",
    "\n",
    "1. [Arrangement Task](#arrangement)\n",
    "    * [Helper functions](#helper-functions)\n",
    "    * [Building stimulus lists](#stim-lists)\n",
    "    * [Preprocessing data](#preprocessing)\n",
    "    * [Turn to dataframe](#turn-to-df)\n",
    "2. [Arrangement Analysis](#arrange-analysis)\n",
    "    * [Distance computations](#distance-comp) \n",
    "    * [Plotting model features](#model-plot)\n",
    "3. [Feature Relationship Computations](#feature-comp) \n",
    "    * [Initial inception setup](#initial-incept) \n",
    "    * [Inception feature extraction](#incept-extract) \n",
    "    * [Initial VGG19 setup](#initial-vgg)\n",
    "    * [VGG19 feature extraction](#vgg-extract) \n",
    "    * [Plotting layer heatmaps](#layer-heatmaps)\n",
    "    * [Plotting relevant layer scatterplots](#layer-scatters)\n",
    "    * [Printing feature correlation values](#feature-corrs)\n",
    "3. [fMRI Data Analysis](#fmri-analysis) \n",
    "    * [Pre-learning Correlation](#prelearning-corr)\n",
    "    * [Comparing to Noise](#prelearning-noise)\n",
    "4. [Visualize Arrangement Trials](#arrangement-visualize) \n",
    "    * [Helper functions](#helpers-visualize)\n",
    "    * [Example visualizations](#example-visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arrangement'></a>\n",
    "# Arrangement Task\n",
    "\n",
    "The first chunk of this notebook analyzes behavioral data from the arrangement task that participants completed. Their task was to drag-and-drop subsets of the iamges until the images placed closest together were the most similar. Each participant completed at least 10 trials, and across these trials, we get distance measures for our critical image pairs. The cells that follow parse this data, perform distance computations, and plot them against our model similarity levels. \n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='helper-functions'></a>\n",
    "## Helper functions\n",
    "\n",
    "The below functions are designed to read, place and populate distance matrices for the 128 synthesized stimuli \n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an item and a given trials dictionary, retrieve the x and y coordinates\n",
    "\n",
    "def retrieve_coords(coord_dict,item):\n",
    "    coords = str(coord_dict[item])\n",
    "    coords = coords.replace(\"'\", \"\")\n",
    "    coords = coords.replace(\", \", \",\")\n",
    "    x = int(coords.split(',')[0])\n",
    "    y = int(coords.split(',')[1])\n",
    "    return x, y\n",
    "\n",
    "# Given a dictionary, return the keys (all items present in the dict)\n",
    "\n",
    "def retrieve_keys(coord_dict):\n",
    "    out_keys = list(coord_dict.keys())\n",
    "    return sorted(out_keys)\n",
    "\n",
    "# Given a dictionary and two critical items, compute the euclidian distance\n",
    "\n",
    "def compute_dist(coord_dict, item1, item2):\n",
    "    x1, y1 = retrieve_coords(coord_dict, item1)\n",
    "    x2, y2 = retrieve_coords(coord_dict, item2)\n",
    "    dist = np.sqrt(np.square(x1-x2)+np.square(y1-y2))\n",
    "    return dist\n",
    "\n",
    "# Create an n by n pandas dataframe, where n is the total number of keys. If full=None, its blank\n",
    "# if full=df, where df is a given dataframe, it will populate it with the values.\n",
    "# This is useful for switching to np.array and back\n",
    "\n",
    "def create_mat(full_keys, full=None):\n",
    "    dist_mat = pd.DataFrame(data=full, index=full_keys, columns=full_keys)\n",
    "    return dist_mat\n",
    "\n",
    "# Given a pandas dataframe, two items, and a distance between them, this function places the distance appropriately\n",
    "\n",
    "def place_dist(fillable_mat, item1, item2, dist):\n",
    "    fillable_mat.loc[item1,item2] = dist\n",
    "    fillable_mat.loc[item2,item1] = dist\n",
    "    #return fillable_mat\n",
    "\n",
    "# Pull a particular distance given a matrix and a pair of items    \n",
    "\n",
    "def pull_dist(fillable_mat, item1, item2):\n",
    "    out_dist = fillable_mat.loc[item1,item2]\n",
    "    return out_dist\n",
    "\n",
    "# Once pd dataframes have been turned into arrays, this compiles them and computes the mean across 3rd dimension\n",
    "# This is because we will have multiple trials, where some have distances for a particular pair and other's don't\n",
    "# If standard, it will standardize the subjects' distances wrt their own judgments\n",
    "\n",
    "def std_array(in_array):\n",
    "    array = np.array(in_array, np.float64)\n",
    "    out_array = (array - np.nanmean(array)) / np.nanstd(array)\n",
    "    return out_array\n",
    "\n",
    "def nan_mean(arrays, standard=False):\n",
    "    all_arrs = np.dstack(arrays)\n",
    "    if standard:\n",
    "        all_arrs = (all_arrs - np.nanmean(all_arrs)) / np.nanstd(all_arrs)\n",
    "    avg_arr = np.nanmean(all_arrs, axis=2)\n",
    "    return avg_arr    \n",
    "\n",
    "def nan_median(arrays, standard=False):\n",
    "    all_arrs = np.dstack(arrays)\n",
    "    if standard:\n",
    "        all_arrs = (all_arrs - np.nanmean(all_arrs)) / np.nanstd(all_arrs)\n",
    "    med_arr = np.nanmedian(all_arrs, axis=2)\n",
    "    return med_arr \n",
    "\n",
    "def subset(fullarray, run, AIX, BIX):\n",
    "    critRun = fullarray[:,run]\n",
    "    critRun = np.swapaxes(critRun, 0, -1)\n",
    "    relCorrs = np.swapaxes(critRun[AIX, BIX], 0, -1)\n",
    "    return relCorrs\n",
    "\n",
    "def shufPrepost(fullarray):\n",
    "    PAs = np.arange(0,15,2)\n",
    "    PBs = np.arange(1,16,2)\n",
    "    AIX = np.random.choice(PAs, 8, replace = False)\n",
    "    BIX = np.random.choice(PBs, 8, replace = False)\n",
    "    #diff = BIX - AIX\n",
    "    #ones = diff[diff == 1]\n",
    "    #if ones.shape[0] > 2:\n",
    "    #    print(AIX, BIX)\n",
    "    pre = subset(fullarray, 0, AIX, BIX)\n",
    "    post = subset(fullarray, 7, AIX, BIX)\n",
    "    return pre, post\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def spot_color(cmap, point=0.5):\n",
    "    cmap = matplotlib.cm.get_cmap(cmap)\n",
    "    rgba = cmap(point)\n",
    "    return rgba\n",
    "\n",
    "def corr_compare(xy, xz, yz, n, twotailed=True, conf_level=0.95):\n",
    "    # Steiger method\n",
    "    # https://psycnet.apa.org/fulltext/1980-08757-001.pdf\n",
    "    \n",
    "    d = xy - xz\n",
    "    determin = 1 - xy * xy - xz * xz - yz * yz + 2 * xy * xz * yz\n",
    "    av = (xy + xz)/2\n",
    "    cube = (1 - yz) * (1 - yz) * (1 - yz)\n",
    "\n",
    "    t2 = d * np.sqrt((n - 1) * (1 + yz)/(((2 * (n - 1)/(n - 3)) * determin + av * av * cube)))\n",
    "    pval = 1 - t.cdf(abs(t2), n - 3)\n",
    "\n",
    "    if twotailed:\n",
    "        pval = pval * 2\n",
    "\n",
    "    return t2, pval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stim-lists'></a>\n",
    "## Building stimulus lists\n",
    "\n",
    "This cell uses the channel axes, similarity levels and A B tags to build a compiled list of all of the possible experimental stimuli.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []\n",
    "\n",
    "chanPairs = ['11-5', '47-34', '56-68', '40-20', '17-85', '79-97', '102-109', '83-101']\n",
    "simLevels = [0, 14, 29, 43, 57, 71, 86, 100]\n",
    "\n",
    "for chanPair in chanPairs:\n",
    "    for simLevel in simLevels:\n",
    "        for pairMate in ['A', 'B']:\n",
    "            all_keys.append('c{}_{}{}'.format(chanPair, simLevel, pairMate))\n",
    "\n",
    "print('compiled {} image file names'.format(len(all_keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## Preprocessing group data\n",
    "\n",
    "1. Reads in the data file, then loop through each line (which should be a single subject)\n",
    "2. Each line is an entire subjects' data, which is then parsed\n",
    "3. Finds the string index where the data from each trial number starts (crit_ind)\n",
    "4. Uses these onsets to loop through and parse the reaction time, build a dictionary of images and their locations\n",
    "5. Runs a bunch of ugly code that makes the dictionary legible and useable for analyses\n",
    "6. Compiles a numpy array containing all the subject IDs, trial numbers, RTs, and dictionaries\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = [open('drag/Batch{}.txt'.format(i)) for i in [1,2,3]]\n",
    "\n",
    "sub_array = []\n",
    "qual_array = []\n",
    "trial_nums = []\n",
    "times = []\n",
    "dicts = []\n",
    "\n",
    "for datafile in datafiles:\n",
    "    # each line is a participant\n",
    "    for line in datafile:\n",
    "        # split each line into the subject identifiers, then the actual data\n",
    "        qual_code = line.split(',')[0]\n",
    "        turk_code = line.split(',')[1]\n",
    "        trials = np.array(line.split(',')[3:])\n",
    "        # count the number of trials\n",
    "        ntrials = line.count('{')\n",
    "        crit_inds=[]\n",
    "        for i in range(1,ntrials+1):\n",
    "            # for each trial, find the relevant string indices where the trial starts\n",
    "            crit = np.in1d(trials, str(i))\n",
    "            crit_ind = [ind for ind in range(len(trials)) if crit[ind]==True][0]\n",
    "            crit_inds.append(crit_ind)\n",
    "        for i in range(len(crit_inds)):\n",
    "            # using thrdr indices, \n",
    "            ind = crit_inds[i]\n",
    "            trial_num = i\n",
    "            time = trials[ind+1]\n",
    "            # Peel away all the nonsense punctuation for each trial\n",
    "            if i < len(crit_inds)-1:\n",
    "                next_ind = crit_inds[i+1]\n",
    "                loc_dict = str(trials[ind+2:next_ind]).split('{')[-1]\n",
    "            else:\n",
    "                loc_dict = str(trials[ind+2:]).split('{')[-1]\n",
    "            \n",
    "            loc_dict = str(loc_dict).split('}')[0]\n",
    "            loc_dict = '{' + loc_dict + '}'\n",
    "            loc_dict = loc_dict.replace(\"\\n\", \"\")\n",
    "            loc_dict = loc_dict.replace(\"' '\", \"', '\")\n",
    "            loc_dict = loc_dict.replace(\"'\", \"\")\n",
    "            loc_dict = json.loads(loc_dict)\n",
    "            \n",
    "            # Appends relevant data to lists.\n",
    "            sub_array.append(turk_code)\n",
    "            qual_array.append(qual_code)\n",
    "            trial_nums.append(trial_num)\n",
    "            times.append(time)\n",
    "            dicts.append(loc_dict)\n",
    "\n",
    "sub_array = np.array(sub_array)\n",
    "qual_array = np.array(qual_array)\n",
    "trial_nums = np.array(trial_nums)\n",
    "times = np.array(times)\n",
    "dicts = np.array(dicts)\n",
    "\n",
    "print('found {} total trials from {} participants'.format(sub_array.shape[0], np.unique(sub_array).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='turn-to-df'></a>\n",
    "## Turn to dataframe\n",
    "\n",
    "This cell turns the data into something more human readable in a dataframe, then displays part of it.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.DataFrame(np.transpose(np.vstack((qual_array, sub_array, trial_nums, times, dicts))), \n",
    "                         columns=['qual','sub','trial','time','coords'])\n",
    "rts = np.array(data_full.loc[:,'time'], dtype=np.int64)\n",
    "rts = rts[:]\n",
    "rts = rts/1000/60\n",
    "print('average trial time = {} minutes'.format(np.around(np.mean(rts), 3)))\n",
    "long_rts = [i for i in range(len(rts)) if rts[i] > 3]\n",
    "\n",
    "data_full.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arrange-analysis'></a>\n",
    "# Arrangement Analysis\n",
    "\n",
    "The first chunk of this notebook analyzes behavioral data from the arrangement task that participants completed. Their task was to drag-and-drop subsets of the iamges until the images placed closest together were the most similar. Each participant completed at least 10 trials, and across these trials, we get distance measures for our critical image pairs. The cells that follow parse this data, perform distance computations, and plot them against our model similarity levels.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance-comp'></a>\n",
    "## Distance computations\n",
    "\n",
    "For each subject:\n",
    "1. We read in their trialwise data and for each trial build an empty 128 x 128 dataframe.\n",
    "2. Cycle through the possible pairs present in this trials.\n",
    "3. Calculate the distance between each image in each pair (either standardized within trial or not).\n",
    "4. Compile dataframes in third dimension across trials.\n",
    "5. Find average for each participant, and count the number of ratings for each pair.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize within trial?\n",
    "standard = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSubs = []\n",
    "subjects = np.unique(data_full['sub'])\n",
    "for si, subject in enumerate(subjects):\n",
    "    arrays = []\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    sub_trials = np.arange(sub_data.shape[0])\n",
    "    for trial in sub_trials:\n",
    "        print(\"subject {}, {}/{}, trial {}/{}\".format(subject, si+1, len(subjects), trial + 1, sub_trials.shape[0]))\n",
    "        clear_output(wait=True)\n",
    "        thisTrial = create_mat(all_keys, full=None)\n",
    "        coord_dict = sub_data.loc[sub_data['trial'] == trial, 'coords'].iloc[0]\n",
    "        key = retrieve_keys(coord_dict)\n",
    "        for i in key:\n",
    "            for j in key:\n",
    "                if i in all_keys and j in all_keys:\n",
    "                    dist_measure = compute_dist(coord_dict, i, j)\n",
    "                    place_dist(thisTrial, i, j, dist_measure)\n",
    "        # standardize the distance within trial\n",
    "        thisTrial = std_array(thisTrial) if standard else thisTrial\n",
    "        arrays.append(np.array(thisTrial))\n",
    "    # this contains this subject's trial set\n",
    "    arrays = np.array(arrays, np.float64)\n",
    "    # quantifying how many ratings have been given per pair\n",
    "    numTrials = np.count_nonzero(~np.isnan(arrays), axis=0)\n",
    "    numTrials = create_mat(all_keys, full=numTrials)\n",
    "    # Getting the average for this subject\n",
    "    subjMean = nan_mean(arrays, standard=False)\n",
    "    subjMeanDF = create_mat(all_keys, full=subjMean)\n",
    "    allSubs.append(np.array(subjMean))\n",
    "    \n",
    "# compiled data from all subjects\n",
    "allSubs = np.array(allSubs, np.float64)\n",
    "# number of ratings across all subjects\n",
    "allCount = np.count_nonzero(~np.isnan(allSubs), axis=0)\n",
    "# mean across subjects\n",
    "allSubsMean = nan_mean(allSubs, standard=False)\n",
    "# piling into dataframes\n",
    "allCountDF = create_mat(all_keys, full=allCount)\n",
    "allSubsMeanDF = create_mat(all_keys, full=allSubsMean)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell reduces the distance values to the values that were set when producing the stimulus set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortOutput = []\n",
    "\n",
    "for impair in range(0, len(all_keys), 2):\n",
    "    A = all_keys[impair]\n",
    "    B = all_keys[impair + 1]\n",
    "    assert A[:len(A)-1] == B[:len(B)-1]\n",
    "    thisDist = pull_dist(allSubsMeanDF, A, B)\n",
    "    sortOutput.append(thisDist)\n",
    "sortOutput = np.array(sortOutput)\n",
    "sortOutput = np.reshape(sortOutput, (8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-plot'></a>\n",
    "## Plotting model features\n",
    "\n",
    "Plot each set of the eight selected endpoints, and all of the eight similarity levels in a heatmap, where each cell is the relative distance between those images in participant behavioral ratings\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "im = ax.matshow(sortOutput)\n",
    "cbar = fig.colorbar(im)\n",
    "\n",
    "ax.set_xticks(np.arange(8))\n",
    "ax.set_xticklabels(np.arange(1,9), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "#bottom, top = ax.get_xlim()\n",
    "#ax.set_xlim(bottom + 0.5, top - 0.5)\n",
    "ax.set_xlabel('Similarity Level', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels(chanPairs, fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "#bottom, top = ax.get_ylim()\n",
    "#ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.set_ylabel('Endpoints', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "\n",
    "cbar.ax.set_ylabel('Pixel Distance', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "for l in cbar.ax.yaxis.get_ticklabels():\n",
    "    l.set_family(\"Arial Narrow\")\n",
    "    \n",
    "    \n",
    "plt.savefig('normDist', dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "print('ACROSS THE TOP: 0 is the most dissimilar (by the model), and 7 is the most similar')\n",
    "print('DOWN THE SIDE: The numbers are meaningless, its simply the different sets of endpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('cool')\n",
    "\n",
    "x = np.tile(np.arange(8), 8)\n",
    "allSubsCorrs = []\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,7), gridspec_kw={'width_ratios': [3, 3, 1]})\n",
    "for sub in range(allSubs.shape[0]):\n",
    "    thisSub = allSubs[sub]\n",
    "    thisSubDF = create_mat(all_keys, full=thisSub)\n",
    "    thisOutput = []\n",
    "\n",
    "    for impair in range(0, len(all_keys), 2):\n",
    "        A = all_keys[impair]\n",
    "        B = all_keys[impair + 1]\n",
    "        assert A[:len(A)-1] == B[:len(B)-1]\n",
    "        thisDist = pull_dist(thisSubDF, A, B)\n",
    "        thisOutput.append(thisDist)\n",
    "    thisOutput = np.array(thisOutput)\n",
    "    thissubCorr = np.corrcoef(np.vstack((x, thisOutput)))[0,1]\n",
    "    allSubsCorrs.append(thissubCorr)\n",
    "    allOutputs = thisOutput if sub == 0 else np.vstack((allOutputs, thisOutput))\n",
    "\n",
    "sortIX = np.argsort(allSubsCorrs)\n",
    "colors = []\n",
    "\n",
    "for si, sub in enumerate(sortIX):\n",
    "    r,g,b,a = cmap(si/allSubs.shape[0])\n",
    "    thisOutput = allOutputs[sub]\n",
    "    z = np.polyfit(x, thisOutput, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1].plot(np.arange(-1,9),p(np.arange(-1,9)),color=(r,g,b,0.4), linewidth=15)\n",
    "    colors.append((r,g,b,0.4))\n",
    "\n",
    "meanOutputs = np.mean(allOutputs, 0)\n",
    "\n",
    "z = np.polyfit(x, meanOutputs, 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(np.arange(-1,9),p(np.arange(-1,9)),color=(0.5,0.5,0.5,0.4), linewidth=15,zorder=-5)\n",
    "axes[0].scatter(x, meanOutputs, 350, color=(1,1,1,1))\n",
    "axes[0].scatter(x, meanOutputs, 350, color=(0.5,0.5,1,0.7))\n",
    "axes[0].set_ylim(0,550)\n",
    "axes[0].set_xlim(-0.5,7.5)\n",
    "axes[0].set_yticks(np.arange(0,551,100))\n",
    "axes[0].set_yticklabels(np.arange(0,551,100), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "axes[0].set_xticks(np.arange(8))\n",
    "axes[0].set_xticklabels(np.arange(1,9), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "axes[0].set_ylabel('Distance (Pixels)', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "axes[0].set_xlabel('Similarity Level', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "Corr = np.corrcoef(np.vstack((x, meanOutputs)))[0,1]\n",
    "axes[0].text(0, 20, 'r = {}'.format(np.around(Corr, 2)), color=(0.5,0.5,1,0.7), \n",
    "             fontsize=64, **{'fontname':'DIN Condensed'})\n",
    "\n",
    "print('Averaged distances -- r = {}'.format(np.around(Corr, 3)))\n",
    "\n",
    "allSubsCorrs = np.array(allSubsCorrs)\n",
    "sortCorrs = allSubsCorrs\n",
    "sortCorrs.sort()\n",
    "axes[1].set_ylim(0,550)\n",
    "axes[1].set_xlim(-0.5,7.5)\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_xticks(np.arange(8))\n",
    "axes[1].set_xticklabels(np.arange(1,9), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "axes[1].set_xlabel('Similarity Level', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "axes[1].text(0, 20, 'r = {}'.format(np.around(np.mean(sortCorrs), 2)), color=(0.5, 0.5, 0.5, 0.5), \n",
    "             fontsize=64, **{'fontname':'DIN Condensed'})\n",
    "axes[1].plot([0,0.35], [75,75], color=(0.5, 0.5, 0.5, 0.5), linewidth=7)\n",
    "\n",
    "boots = []\n",
    "for i in range(10000):\n",
    "    IX = np.random.choice(sortCorrs.shape[0], sortCorrs.shape[0])\n",
    "    this = sortCorrs[IX]\n",
    "    boots.append(np.mean(this))\n",
    "print('Across participants -- M = {}, CI95 = [{} {}]'.format(np.around(np.mean(boots), 3), \n",
    "                                                             np.around(np.percentile(boots, 2.5), 3),\n",
    "                                                             np.around(np.percentile(boots, 97.5), 3)))\n",
    "\n",
    "axes[2].set_ylim(-0.5,29.5)\n",
    "axes[2].barh(np.arange(allSubs.shape[0]), sortCorrs, color=colors)\n",
    "axes[2].set_yticks(np.arange(-0.8,-0.31))\n",
    "axes[2].set_yticklabels(np.arange(0,551,100), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "axes[2].spines['top'].set_visible(False)\n",
    "axes[2].spines['left'].set_visible(False)\n",
    "axes[2].set_xticks(np.arange(-0.8,0.01,0.2))\n",
    "axes[2].set_xticklabels(np.around(np.arange(-0.8,0.01,0.2),2), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "axes[2].set_xlabel('Correlation', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "axes[2].set_ylabel('Individual Participants', fontsize=28, labelpad=30, rotation=270, **{'fontname':'Arial Narrow'})\n",
    "axes[2].yaxis.set_label_position(\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/distanceVal.pdf', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-comp'></a>\n",
    "# Feature Correlation Computations\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLoc = '../synthesis'\n",
    "imageLoc = 'stim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chanPairs = ['11-5', '47-34', '56-68', '40-20', '17-85', '79-97', '102-109', '83-101']\n",
    "simLevels = [0, 14, 29, 43, 57, 71, 86, 100]\n",
    "\n",
    "Labels = []\n",
    "\n",
    "for chanPair in chanPairs:\n",
    "    for simLevel in simLevels:\n",
    "        for pairMate in ['A', 'B']:\n",
    "            Labels.append('{}/c{}_{}{}.png'.format(imageLoc, chanPair, simLevel, pairMate))\n",
    "\n",
    "print('compiled {} image file names'.format(len(Labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(layers):\n",
    "    activations = []\n",
    "    for layer in layers:\n",
    "        thisLayer = graph.get_tensor_by_name(\"import/{}:0\".format(layer))\n",
    "        activations.append(thisLayer)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial-incept'></a>\n",
    "## Initial Inception setup\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = '{}/tensorflow_inception_graph.pb'.format(modelLoc)\n",
    "\n",
    "# creating TensorFlow session and loading the model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), graph.device('/cpu:0'), tf.Session() as sess:\n",
    "    with tf.gfile.FastGFile(model_fn, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    t_input = tf.placeholder(np.float32, name='input') # define the input tensor\n",
    "    imagenet_mean = 117.0\n",
    "    t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n",
    "    tf.import_graph_def(graph_def, {'input':t_preprocessed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n",
    "\n",
    "layers1 = [l.split('/')[1] for l in layers if l.split('/')[1].split('2')[0] == 'conv']\n",
    "layers2 = [l.split('/')[1] for l in layers if 'pool_reduce' in l]\n",
    "\n",
    "incept_layers = layers1 + layers2\n",
    "print(len(incept_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOALS = np.zeros((len(incept_layers), len(simLevels)))\n",
    "GOALS[:8, :] = 0.25\n",
    "GOALS[8:, :] = np.arange(0, 1.01, 1/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='incept-extract'></a>\n",
    "## Inception feature extraction\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRS = np.zeros((len(incept_layers),len(simLevels),len(chanPairs)))\n",
    "with graph.as_default(), graph.device('/cpu:0'), tf.Session() as sess:\n",
    "    acts = []\n",
    "    for i, imfile in enumerate(Labels):\n",
    "        print('compiling image {}/{} for inception'.format(i + 1, len(Labels)))\n",
    "        clear_output(wait=True)\n",
    "        img0 = np.float32(Image.open(imfile).convert(\"RGB\"))\n",
    "        act = sess.run(get_tensor(incept_layers), {t_input:img0})\n",
    "        acts.append(act)\n",
    "    for lnum, layer in enumerate(incept_layers):\n",
    "        for anum, axis in enumerate(np.arange(0, 128, 16)):\n",
    "            print('computing correlation for layer {}/{}, axis {}/{}'.format(lnum + 1, len(incept_layers), \n",
    "                                                                            anum + 1, len(chanPairs)))\n",
    "            clear_output(wait=True)\n",
    "            for imnum in range(axis, axis+16):\n",
    "                thisact = acts[imnum][lnum]\n",
    "                thislayer = thisact if imnum  == axis else np.concatenate((thislayer, thisact))\n",
    "        \n",
    "            thislayer = thislayer.reshape(16, -1)\n",
    "            corrs = np.corrcoef(thislayer)\n",
    "            relcorrs = corrs[np.arange(0,16,2), np.arange(1,17,2)]\n",
    "            CORRS[lnum, :, anum] = relcorrs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial-vgg'></a>\n",
    "## Initial VGG19 setup\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=[base_model.get_layer('block1_pool').output, \n",
    "                                                base_model.get_layer('block2_pool').output, \n",
    "                                                base_model.get_layer('block3_pool').output, \n",
    "                                                base_model.get_layer('block4_pool').output, \n",
    "                                                base_model.get_layer('block5_pool').output, \n",
    "                                                base_model.get_layer('fc1').output, \n",
    "                                                base_model.get_layer('fc2').output, \n",
    "                                                base_model.get_layer('predictions').output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vgg-extract'></a>\n",
    "## VGG feature extraction\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.empty((0, 224, 224, 3), float)\n",
    "for i, imfile in enumerate(Labels):\n",
    "    print('compiling image {}/{} for VGG19'.format(i + 1, len(Labels)))\n",
    "    clear_output(wait=True)\n",
    "    img = image.load_img(imfile, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    batch = np.concatenate((batch, x))\n",
    "print('making predictions')   \n",
    "b1p, b2p, b3p, b4p, b5p, fc1, fc2, pred = model.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19 = np.zeros((7,len(simLevels),len(chanPairs)))\n",
    "for lnum, layer in enumerate([b1p, b2p, b3p, b4p, b5p, fc1, fc2]):\n",
    "    print(layer.shape)\n",
    "    for anum, axis in enumerate(np.arange(0, 128, 16)):\n",
    "        print('computing correlation for layer {}/{}, axis {}/{}'.format(lnum + 1, 7, \n",
    "                                                                        anum + 1, len(chanPairs)))\n",
    "        clear_output(wait=True)\n",
    "        thisact = layer[axis: axis+16]\n",
    "        thislayer = thisact.reshape(16, -1)\n",
    "        corrs = np.corrcoef(thislayer)\n",
    "        relcorrs = corrs[np.arange(0,16,2), np.arange(1,17,2)]\n",
    "        VGG19[lnum, :, anum] = relcorrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layer-heatmaps'></a>\n",
    "## Plotting layer heatmaps\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptNames = ['2D0', '2D1', '2D2', '3A', '3B', '4A', '4B', '4C', '4D', '4E', '5A', '5B']\n",
    "VGGNames = ['B1P', 'B2P', 'B3P', 'B4P', 'B5P', 'FC1', 'FC2']\n",
    "layerNames = [inceptNames, inceptNames, VGGNames] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(24,8))\n",
    "for i, (ax, source) in enumerate(zip(axes, [GOALS, CORRS, VGG19])):\n",
    "    toPlot = source if len(source.shape) ==2 else np.mean(source, 2)\n",
    "    im = ax.imshow(toPlot, vmin=0, vmax=1, cmap='cool')\n",
    "    ax.set_xticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(1,9), fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "    if i == 5:\n",
    "        ax.set_yticks([])\n",
    "        ax.set_yticklabels([])\n",
    "    else:\n",
    "        ax.set_yticks(np.arange(len(layerNames[i])))\n",
    "        bottom, top = ax.get_ylim()\n",
    "        #ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "        ax.set_yticklabels(layerNames[i], fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "    \n",
    "fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.8,\n",
    "                    wspace=0.2, hspace=0.2)\n",
    "\n",
    "cb_ax = fig.add_axes([0.83, 0.1, 0.02, 0.8])\n",
    "cbar = fig.colorbar(im, cax=cb_ax)\n",
    "\n",
    "cbar.ax.tick_params(labelsize=28)\n",
    "for l in cbar.ax.yaxis.get_ticklabels():\n",
    "    l.set_family(\"Arial Narrow\")\n",
    "cbar.set_ticks(np.arange(0, 1.1, 0.25))\n",
    "cbar.set_ticklabels(np.arange(0, 1.1, 0.25))\n",
    "\n",
    "plt.savefig('figures/modelVal.pdf', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layer-scatters'></a>\n",
    "## Plotting relevant layer scatterplots\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colorDict = dict(zip([8,9,10,11], [(218, 67, 68), (241,161,104), (78, 128, 130), (79, 200, 120)]))\n",
    "blankcols = []\n",
    "for x in [0.35, 0.45, 0.55, 0.65]:\n",
    "    r,g,b,a = spot_color('cool', x)\n",
    "    blankcols.append((r*255,g*255,b*255))\n",
    "colorDict = dict(zip([8,9,10,11], blankcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16,5), sharex=True, sharey=True)\n",
    "for ax, layerIX in zip(axes, range(8,12)):\n",
    "    rr,gg,bb = colorDict[layerIX]\n",
    "    rr, gg, bb = rr/255, gg/255, bb/255\n",
    "    for axis in range(CORRS.shape[2]):\n",
    "        ax.scatter(GOALS[layerIX], CORRS[layerIX,:,axis], 150, alpha=0.4, color=(rr,gg,bb))\n",
    "    if layerIX == 8:\n",
    "        ax.set_ylabel('Inception Outcome', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "        ax.set_yticks(np.arange(0,1.01, 0.25))\n",
    "        ax.set_yticklabels(np.arange(0,1.01, 0.25), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "    ax.set_xticks(np.arange(0,1.01, 0.25))\n",
    "    ax.set_xticklabels(np.arange(0,1.01, 0.25), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "    ax.text(0.8, 0, inceptNames[layerIX], color=(rr,gg,bb,1), fontsize=64, **{'fontname':'DIN Condensed'})\n",
    "\n",
    "fig.text(0.5, -0.05, 'Correlation Goal', ha='center', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylim(-0.02,1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/modelCorr.pdf', dpi=600, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-corrs'></a>\n",
    "## Printing feature correlation values\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress, t, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('INCEPTION')\n",
    "for layerIX in range(8):\n",
    "    diffs = []\n",
    "    print()\n",
    "    print('------------ layer {} ------------'.format(layerIX+1))\n",
    "    allX = np.repeat(GOALS[9][:, np.newaxis], 8, axis=1).flatten()\n",
    "    allY = CORRS[layerIX].flatten()\n",
    "    forCorr = np.vstack((allX, allY))\n",
    "    for j in range(allY.shape[0]):\n",
    "        for k in range(j+1, allY.shape[0]):\n",
    "            diff = np.absolute(allY[j] - allY[k])\n",
    "            diffs.append(diff)\n",
    "    print('all points r = {}, SD = {}, all points d = {}, CI95 = [{} {}]'.format(np.around(np.corrcoef(forCorr)[0,1],3),\n",
    "                                                                                 np.around(np.std(allY), 3),\n",
    "                                                                                 np.around(np.mean(diffs),3),\n",
    "                                                                                 np.around(np.percentile(diffs, 2.5),3),\n",
    "                                                                                 np.around(np.percentile(diffs, 97.5),3)))\n",
    "    \n",
    "    for axis in range(CORRS.shape[2]):\n",
    "        axdiffs = []\n",
    "        thisY = CORRS[layerIX,:,axis]\n",
    "        for j in range(thisY.shape[0]):\n",
    "            for k in range(j+1, allY.shape[0]):\n",
    "                diff = np.absolute(thisY[j] - allY[k])\n",
    "                axdiffs.append(diff)\n",
    "        print('-----axis {}, d = {}, CI95 = [{} {}]'.format(axis+1,np.around(np.mean(axdiffs),3),\n",
    "                                                                    np.around(np.percentile(axdiffs, 2.5),3),\n",
    "                                                                    np.around(np.percentile(axdiffs, 97.5),3)))\n",
    "\n",
    "\n",
    "for layerIX in range(8,12):\n",
    "    print()\n",
    "    print('------------ layer {} ------------'.format(layerIX+1))\n",
    "    allX = np.repeat(GOALS[layerIX][:, np.newaxis], 8, axis=1).flatten()\n",
    "    allY = CORRS[layerIX].flatten()\n",
    "    forCorr = np.vstack((allX, allY))\n",
    "    avgX = GOALS[layerIX]\n",
    "    avgY = np.mean(CORRS[layerIX], 1)\n",
    "    forCorr2 = np.vstack((avgX, avgY))\n",
    "    for j in range(allY.shape[0]):\n",
    "        for k in range(j+1, allY.shape[0]):\n",
    "            diff = np.absolute(allY[j] - allY[k])\n",
    "            diffs.append(diff)\n",
    "    print('all points r = {}, avg = {}, SD = {}, all points d = {}'.format(np.around(np.corrcoef(forCorr)[0,1],3),\n",
    "                                                                           np.around(np.corrcoef(forCorr2)[0,1],3),\n",
    "                                                                           np.around(np.std(allY), 3), \n",
    "                                                                           np.around(np.mean(diffs),3)))\n",
    "    \n",
    "    for axis in range(CORRS.shape[2]):\n",
    "        thisX = GOALS[layerIX]\n",
    "        thisY = CORRS[layerIX,:,axis]\n",
    "        forCorr = np.vstack((thisX, thisY))\n",
    "        print('-----axis {}, r = {}'.format(axis+1,np.around(np.corrcoef(forCorr)[0,1],3)))\n",
    "\n",
    "        \n",
    "print()\n",
    "print('VGG19')\n",
    "for layerIX in range(7):\n",
    "    print()\n",
    "    print('------------ layer {} ------------'.format(layerIX+1))\n",
    "    allX = np.repeat(GOALS[9][:, np.newaxis], 8, axis=1).flatten()\n",
    "    allY = VGG19[layerIX].flatten()\n",
    "    forCorr = np.vstack((allX, allY))\n",
    "    avgX = GOALS[9]\n",
    "    avgY = np.mean(VGG19[layerIX], 1)\n",
    "    forCorr2 = np.vstack((avgX, avgY))\n",
    "    print('all points r = {}, avg = {}, SD = {}'.format(np.around(np.corrcoef(forCorr)[0,1],3), \n",
    "                                                        np.around(np.corrcoef(forCorr2)[0,1],3),\n",
    "                                                        np.around(np.std(allY), 3)))\n",
    "    for axis in range(CORRS.shape[2]):\n",
    "        thisX = GOALS[9]\n",
    "        thisY = VGG19[layerIX,:,axis]\n",
    "        forCorr = np.vstack((thisX, thisY))\n",
    "        print('-----axis {}, r = {}'.format(axis+1,np.around(np.corrcoef(forCorr)[0,1],3)))\n",
    "    \n",
    "for layerIX in range(4):\n",
    "    for layerIX2 in range(4, 7):\n",
    "        allX = np.repeat(GOALS[9][:, np.newaxis], 8, axis=1).flatten()\n",
    "        allY = VGG19[layerIX].flatten()\n",
    "        allZ = VGG19[layerIX2].flatten()\n",
    "        forCorr = np.corrcoef(np.vstack((allX, allY, allZ)))\n",
    "        tstat, pstat = corr_compare(forCorr[0,1], forCorr[0,2], forCorr[1,2], 64, twotailed=True, conf_level=0.95)\n",
    "        print('difference between layers {} ({}) and {} ({}): r val btw = {}: t = {}, p ='.format(layerIX + 1, \n",
    "                                                                                                  np.around(forCorr[0,1],5),\n",
    "                                                                                                  layerIX2 + 1, \n",
    "                                                                                                  np.around(forCorr[0,2],5),\n",
    "                                                                                                  np.around(forCorr[1,2],5),\n",
    "                                                                                                  np.around(tstat,3), \n",
    "                                                                                                  pstat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fmri-analysis'></a>\n",
    "# fMRI Analysis\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelearning-corr'></a>\n",
    "## Pre-learning correlation\n",
    "The following analyses find the representational change at each similarity level, quantifying the true effect in the sample, then shuffle AB pairings as many times as is dictated above and store these values.\n",
    "\n",
    "This step reads in one of two numpy arrays, the first contains only the correlations for the relevant image pairings, while the other contains the entire 16 x 16 imagewise correlation matrix for every run.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the ROIs to conduct the analysis in\n",
    "roiList = ['V1', 'V2', 'LOC', 'Fus', 'PHC', 'IT', 'EC', 'perirhinal']  \n",
    "resultDict36 = {}\n",
    "noiseDict36 = {}\n",
    "resultDict41 = {}\n",
    "noiseDict41 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base file names for the two types of numpy arrays - these are filled later in the code\n",
    "critBase = '{}/allsubs_allruns_{}.npy'\n",
    "allBase = '{}/allims_runs_{}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList[:]):\n",
    "\n",
    "    # This file is only the relevant correlations\n",
    "    CritPairs = np.load(critBase.format('surf', roi))\n",
    "    print('Running {}'.format(roi))\n",
    "\n",
    "    # This computes the true effect in the whole sample.\n",
    "    all_pre = CritPairs[:,0]\n",
    "    print(all_pre.shape)\n",
    "\n",
    "    # This saves the output to a csv file\n",
    "    outFile = pd.DataFrame(all_pre)\n",
    "    outFile.to_csv('./csvout/{}_surf.csv'.format(roi))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rn, roi in enumerate(roiList):\n",
    "    print('Running {}'.format(roi))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # read in the relevant data, subset to only the critical values\n",
    "    dat = pd.read_csv('./csvout/{}_surf.csv'.format(roi))\n",
    "    raw = np.array(dat.iloc[:,1:])\n",
    "\n",
    "    # convert to pandas dataframe suitable for this analysis\n",
    "    melted = pd.melt(dat, id_vars='Unnamed: 0', var_name='simLevel', value_name='Change')\n",
    "    melted.rename(columns={'Unnamed: 0':'subID'}, inplace=True)\n",
    "    rvals = []\n",
    "\n",
    "    for subject in range(raw.shape[0]):\n",
    "        # compute actual ~ predicted correlation for a given held out subject\n",
    "        corr = np.corrcoef(np.vstack((np.arange(8), raw[subject])))[0,1]\n",
    "        # Compile subjects in list\n",
    "        rvals.append(corr)\n",
    "    rvals = np.array(rvals)\n",
    "    RVALS = rvals if rn == 0 else np.vstack((RVALS, rvals))\n",
    "    print(RVALS.shape)\n",
    "    us = np.zeros((shuffles))\n",
    "    # Bootstrap resample a number of times, find 95% CI\n",
    "    for shuf in range(shuffles):\n",
    "        print('Running {} -- {}/{}'.format(roi, shuf+1, shuffles))\n",
    "        clear_output(wait=True)\n",
    "        test = np.random.choice(rvals.shape[0], rvals.shape[0])\n",
    "        u = np.mean(rvals[test])\n",
    "        us[shuf] = u\n",
    "    # Fisher transform for statistical analysis\n",
    "    us = np.arctanh(us)\n",
    "    LB = np.percentile(us, 2.5)\n",
    "    U = np.mean(us)\n",
    "    UB = np.percentile(us, 97.5)\n",
    "    resultDict41['{}'.format(roi)] = [LB, U, UB]\n",
    "#np.save(\"VisSimR.npy\", RVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    LB, U, UB = resultDict41['{}'.format(roi)]\n",
    "    print('For {} -- M = {}, 95\\% CI = [{} {}]'.format(roi, \n",
    "                                                    np.around(U, 3),\n",
    "                                                    np.around(LB, 3), \n",
    "                                                    np.around(UB, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelearning-noise'></a>\n",
    "## Comparing to noise\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList[:]):\n",
    "    # This is every intercorrelation (16 x 16)\n",
    "    AllIms = np.load(allBase.format('surf', roi))\n",
    "    us = np.zeros((shuffles))\n",
    "    for shuf in range(shuffles):\n",
    "        print('Running {} -- {}/{}'.format(roi, shuf+1, shuffles))\n",
    "        clear_output(wait=True)\n",
    "        all_pre, _ = shufPrepost(AllIms)  \n",
    "        rvals = []\n",
    "        for subject in range(all_pre.shape[0]):\n",
    "            # compute correlation for a given subject\n",
    "            corr = np.corrcoef(np.vstack((np.arange(8), all_pre[subject])))[0,1]\n",
    "            # Compile subjects in list\n",
    "            rvals.append(corr)\n",
    "        us[shuf] = np.mean(np.array(rvals))\n",
    "    us = np.arctanh(us)\n",
    "    _, trueU, _ = resultDict41['{}'.format(roi)]\n",
    "    percentile = (us > trueU).sum() / shuffles\n",
    "    LB = np.percentile(us, 2.5)\n",
    "    U = np.mean(us)\n",
    "    UB = np.percentile(us, 97.5)\n",
    "    noiseDict41['{}'.format(roi)] = [LB, U, UB, percentile]\n",
    "    plt.hist(us,100)\n",
    "    plt.axvline(LB)\n",
    "    plt.axvline(UB)\n",
    "    #plt.savefig('figures/noise_{}.pdf'.format(roi), dpi=600)\n",
    "    plt.show()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    LB, U, UB, percentile = noiseDict41['{}'.format(roi)]\n",
    "    print('For {} -- M = {}, CI95 = [{} {}], percentile = {}'.format(roi,\n",
    "                                                                     np.around(U, 4),\n",
    "                                                                     np.around(LB, 4), \n",
    "                                                                     np.around(UB, 4),\n",
    "                                                                     np.around(percentile, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottingDict = {}\n",
    "\n",
    "masterROIs = ['V1', 'V2', 'LOC', 'Fus', 'PHC', 'IT', 'EC', 'perirhinal']\n",
    "masterCOLs = [(29, 157, 119), (217, 95, 2), (117, 112, 179), (231, 41, 138), \n",
    "              (101, 166, 30), (230, 171, 0), (165, 118, 28), (102, 102, 102)]\n",
    "masterNOM = ['V1', 'V2', 'LO', 'FG', 'PHC', 'IT', 'EC', 'PRC']\n",
    "nameDict = dict(zip(masterROIs, masterNOM))\n",
    "colorDict = dict(zip(masterROIs, masterCOLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    dat = np.array(pd.read_csv('./csvout/{}_surf.csv'.format(roi)))[:, 1:]\n",
    "    us = np.zeros((shuffles, 8))\n",
    "    for shuf in range(shuffles):\n",
    "        IX = np.random.choice(dat.shape[0], dat.shape[0])\n",
    "        test = dat[IX, :]\n",
    "        us[shuf] = (np.mean(test, 0))\n",
    "    LB = np.percentile(us, 2.5, 0)\n",
    "    U = np.mean(us, 0)\n",
    "    UB = np.percentile(us, 97.5, 0)\n",
    "    plottingDict['{}'.format(roi)] = [LB, U, UB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    r,g,b = colorDict[roi]\n",
    "    r, g, b = r/255, g/255, b/255\n",
    "    LB, U, UB = plottingDict['{}'.format(roi)]\n",
    "    fig, ax = plt.subplots(figsize=(6,8))\n",
    "    plt.fill_between(np.arange(8), LB, UB, color = (1, 1, 1, 0.7), zorder=-2, lw=0)\n",
    "    plt.fill_between(np.arange(8), LB, UB, color = (r, g, b, 0.1), zorder=-2, lw=0)\n",
    "    plt.plot(U, lw=4, c=[r,g,b,1])\n",
    "    plt.scatter(np.arange(8), U, s=150, lw=1.5, edgecolor=[[r,g,b,1]], facecolor=[[r,g,b,0.6]])\n",
    "    plt.xticks(np.arange(8), np.arange(1,9), fontsize=18, **{'fontname':'Arial Narrow'})\n",
    "    plt.ylabel('Representational Change', fontsize=24, **{'fontname':'Arial Narrow'})\n",
    "    plt.xlabel('Similarity Level', fontsize=24, **{'fontname':'Arial Narrow'})\n",
    "    #plt.savefig('figures/simPattern_{}.pdf'.format(roi), dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_params(bottom, top):\n",
    "    fullRange = top - bottom\n",
    "    roundBot = np.around(bottom * 20)/20\n",
    "    roundTop = np.around(top * 20)/20\n",
    "    textLoc = roundBot + (0.03 * fullRange)\n",
    "    return roundBot, roundTop, textLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roiList = ['V1', 'V2', 'LOC', 'IT', 'Fus', 'PHC']\n",
    "sig = ['', '', '*', '', '', '*']\n",
    "sigdict = dict(zip(roiList, sig))\n",
    "colorVals = np.arange(0,1.01,1/len(roiList))\n",
    "print(colorVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(roiList), figsize=(int(len(roiList)*3.5), 8), sharey=False)\n",
    "\n",
    "for i, (ax, roi) in enumerate(zip(axes, roiList)):\n",
    "    r,g,b,a = spot_color('cool', colorVals[i])\n",
    "    r, g, b = r/1.1, g/1.1, b/1.1\n",
    "    LB, U, UB = plottingDict['{}'.format(roi)]\n",
    "    ax.fill_between(np.arange(8), LB, UB, color = (1, 1, 1, 0.7), zorder=-2, lw=0)\n",
    "    ax.fill_between(np.arange(8), LB, UB, color = (r, g, b, 0.1), zorder=-2, lw=0)\n",
    "    ax.plot(U, lw=4, c=[r,g,b,1])\n",
    "    ax.scatter(np.arange(8), U, s=150, lw=1.5, edgecolor=[[r,g,b,1]], facecolor=[[r,g,b,0.6]])\n",
    "    ax.set_xticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(1,9), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "    ax.set_xlabel('Model Similarity Level', color=(0, 0, 0, 0), fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "    bottom, top = ax.get_ylim()\n",
    "    newbot, newtop, textloc = get_plot_params(bottom, top)\n",
    "    ax.text(0, textloc, '{}{}'.format(nameDict[roi], sigdict[roi]), color=(r,g,b,1), fontsize=64, **{'fontname':'DIN Condensed'})\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Neural Similarity', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "    ax.set_yticks(np.around(np.arange(newbot, newtop+.01, 0.05),2))\n",
    "    ax.set_yticklabels(np.around(np.arange(newbot, newtop+.01, 0.05),2),\n",
    "                       fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "    ax.set_ylim(newbot, top)\n",
    "\n",
    "fig.text(0.5, 0.02, 'Model Similarity Level', ha='center', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/manuscript_vis.pdf', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arrangement-visualize'></a>\n",
    "# Visualize Arrangement Trials\n",
    "\n",
    "[(back to top)](#TOC)\n",
    "\n",
    "What follows are some functions and code to produce visualizations of a given arrangement trial or trials, for a given participant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='helpers-visualize'></a>\n",
    "## Helper functions\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproduce_trial(subject, trial, save=False, show=False):\n",
    "    master = Image.new(\"RGBA\", (1000,1000), (255,255,255,0))\n",
    "    draw = ImageDraw.Draw(master)\n",
    "    draw.ellipse((88, 88, 912, 912), fill=(0,0,0))\n",
    "    draw.ellipse((96, 96, 904, 904), fill=(255,255,255))\n",
    "    draw.ellipse((104, 104, 896, 896), fill=(0,0,0))\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    coord_dict = sub_data.loc[sub_data['trial'] == trial, 'coords'].iloc[0] \n",
    "    rt = sub_data.loc[sub_data['trial'] == trial, 'time'].iloc[0] \n",
    "    keys = retrieve_keys(coord_dict)\n",
    "    for key in keys:\n",
    "        x, y = retrieve_coords(coord_dict,key)\n",
    "        x -= 100\n",
    "        y -= 100\n",
    "        currIm = Image.open('stim/{}.png'.format(key))\n",
    "        currIm.thumbnail((120,120))\n",
    "        master.paste(currIm, (x,y), currIm)\n",
    "    master = master.resize((2000, 2000), Image.ANTIALIAS)\n",
    "    arrangement = np.asarray(master)\n",
    "    if show:\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(arrangement)\n",
    "        plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(str(subject)+'_'+str(trial)+'.pdf', dpi=1000)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return rt, arrangement\n",
    "    \n",
    "def giffify_trials(subject, time_per_trial='match', total_time=20, verbose=False):\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    sub_trials = sub_data.shape[0]\n",
    "    images=[]\n",
    "    rts=[]\n",
    "    for trial in range(sub_trials):\n",
    "        clear_output(wait=True) if verbose else print('',end='')\n",
    "        print('subject: {}, trial {}/{} done'.format(subject, trial+1, sub_trials)) if verbose else print('',end='')\n",
    "        rt, arrangement = reproduce_trial(subject, trial)\n",
    "        images.append(arrangement)\n",
    "        rts.append(int(rt))\n",
    "    if time_per_trial == 'match':\n",
    "        whole_time = np.sum(rts)\n",
    "        times = list((np.array(rts) / whole_time) * 20)\n",
    "    else:\n",
    "        times = time_per_trial\n",
    "    imageio.mimsave(str(subject)+'_all.gif', images, duration=times)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example-visualize'></a>\n",
    "## Example visualizations\n",
    "\n",
    "This cell will produce a gif of all trials for one partipant, then a single trial visualization for another.\n",
    "\n",
    "[(back to top)](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giffify_trials('26HoFBT4', verbose=True)\n",
    "rt, arrangement = reproduce_trial('5kwO4weo', 1, save=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import ttest_1samp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "from IPython.display import clear_output\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Learning Analyses\n",
    "\n",
    "1. [Helper functions](#helper-functions)\n",
    "2. [Initial setup](#initial-setup) \n",
    "    * [Finding subjects](#finding-subjects)\n",
    "3. [Learning analysis](#learning-analysis) \n",
    "    * [Setting parameters](#set-parameters)\n",
    "    * [ROI effects](#main-effects)\n",
    "    * [DG effect](#DG-effects)\n",
    "    * [NMPH Model fitting](#nmph-model)\n",
    "4. [Comparing to noise](#noise-dist) \n",
    "5. [Plotting results](#plotting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='helper-functions'></a>\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(fullarray, run, AIX, BIX):\n",
    "    critRun = fullarray[:,run]\n",
    "    critRun = np.swapaxes(critRun, 0, -1)\n",
    "    relCorrs = np.swapaxes(critRun[AIX, BIX], 0, -1)\n",
    "    return relCorrs\n",
    "\n",
    "def shufPrepost(fullarray):\n",
    "    PAs = np.arange(0,15,2)\n",
    "    PBs = np.arange(1,16,2)\n",
    "    AIX = np.random.choice(PAs, 8, replace = False)\n",
    "    BIX = np.random.choice(PBs, 8, replace = False)\n",
    "    pre = subset(fullarray, 0, AIX, BIX)\n",
    "    post = subset(fullarray, 7, AIX, BIX)\n",
    "    return pre, post\n",
    "\n",
    "def func(x, a, b, c, d):\n",
    "    return a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "def lv1outPoly(DF, subject, constrain=False, plot=False, order=3):\n",
    "    # Isolate single subject ...\n",
    "    thisSub = DF[DF['subID'] == subject]\n",
    "    # ... from the remaining subjects\n",
    "    lv1out = DF[DF['subID'] != subject]\n",
    "    # Assign as training (n - 1 subjects) and test (held out subject) data\n",
    "    trainy = np.asarray(lv1out['Change'])\n",
    "    trainx = np.asarray(pd.to_numeric(lv1out['simLevel']))\n",
    "    testy = np.asarray(thisSub['Change'])\n",
    "    testx = np.asarray(pd.to_numeric(thisSub['simLevel']))\n",
    "\n",
    "    if constrain:\n",
    "        weights, _ = curve_fit(func, trainx, trainy, bounds=([-np.inf, -np.inf, -np.inf, 0], [np.inf, np.inf, np.inf, np.inf]))\n",
    "        weights = weights[::-1]\n",
    "    else: \n",
    "        # fit the specified order polynomial\n",
    "        weights = np.polyfit(trainx, trainy, order)\n",
    "    \n",
    "    # Use the weights to build a model\n",
    "    model = np.poly1d(weights)\n",
    "    # Compute correlation between actual and predicted data\n",
    "    if plot:\n",
    "        plt.plot(np.arange(8), model(np.arange(8)))\n",
    "    correspondence = np.corrcoef(np.vstack((testy, model(testx))))[0,1]\n",
    "    return correspondence\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial-setup'></a>\n",
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='finding-subjects'></a>\n",
    "#### Finding subjects\n",
    "This cell lists the files in the current directory, and then subsets them to include only subjects relevant to the two relevant analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile subjects\n",
    "fullList = [file.split(\".\")[0] for file in os.listdir(\".\") if file.split(\".\")[-1] == \"gz\" and 'sub' in file]\n",
    "fullList.sort()\n",
    "subList = [i for i in fullList if i not in [\"sub-23\", \"sub-38\", \"sub-39\", \"sub-40\", \"sub-41\"]]\n",
    "visList = fullList\n",
    "\n",
    "print('{} subjects for learning analysis '.format(len(subList)))\n",
    "print('{} subjects for visual analysis'.format(len(visList)))\n",
    "print(subList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='learning-analysis'></a>\n",
    "## Learning analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='set-parameters'></a>\n",
    "#### Setting initial parameters\n",
    "* Choose the parameters in this first cell. First, establish whether you want to do this analysis in a single hemisphere or collapsed across both, or all three.\n",
    "* Then, which ROIs you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which hemis to run the analysis in\n",
    "runTypes = ['both']\n",
    "# List the ROIs to conduct the analysis in\n",
    "roiList = ['hipp', 'ca1', 'ca23', 'dg']  # ['hipp', 'ca1', 'ca23', 'dg', 'ca23dg', 'sub']\n",
    "#roiList = ['PHC', 'perirhinal', 'EC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These establish a dictionary to appropriately name files, provide a filename base to later fill depending on subject, ROI and hemishpere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterROIs = ['hipp', 'ca1', 'ca23', 'dg', 'ca23dg', 'sub', \n",
    "              'phc', 'prc', 'ec', 'PHC', 'perirhinal', 'EC']\n",
    "masterCOLs = [(218, 67, 68), (241,161,104), (78, 128, 130), \n",
    "              (79, 200, 120), (50,2,31), (10,36,99), \n",
    "              (13, 29, 118), (51, 130, 121), (91, 160, 74),\n",
    "              (13, 29, 118), (51, 130, 121), (91, 160, 74)]\n",
    "masterNOM = ['HC', 'CA1', 'CA2/3', 'DG', 'CA2/3/DG', 'SUB', \n",
    "             'PHC', 'PRC', 'EC', 'PHC', 'PRC', 'EC']\n",
    "nameDict = dict(zip(masterROIs, masterNOM))\n",
    "colorDict = dict(zip(masterROIs, masterCOLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of types of analysis to do, total set is left hemi, right hemi, then combined across\n",
    "types = ['left', 'right', 'both']\n",
    "stems = ['_left', '_right', '']\n",
    "stemdict = dict(zip(types, stems))\n",
    "\n",
    "# Base file names for the two types of numpy arrays - these are filled later in the code\n",
    "critBase = '{}/allsubs_allruns{}_{}.npy'\n",
    "allBase = '{}/allims_runs{}_{}.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='main-effects'></a>\n",
    "#### Computing the main effects in a given ROI\n",
    "The following analyses find the representational change at each similarity level, quantifying the true effect in the sample, then shuffle AB pairings as many times as is dictated above and store these values.\n",
    "\n",
    "This step reads in one of two numpy arrays, the first contains only the correlations for the relevant image pairings, while the other contains the entire 16 x 16 imagewise correlation matrix for every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hemi in runTypes:\n",
    "    stem = stemdict[hemi]\n",
    "    for i, roi in enumerate(roiList[:]):\n",
    "        \n",
    "        # This file is only the relevant correlations\n",
    "        CritPairs = np.load(critBase.format('surf', stem, roi))\n",
    "        print('Running {} hemisphere {}'.format(hemi, roi))\n",
    "        \n",
    "        # This computes the true effect in the whole sample.\n",
    "        all_pre = CritPairs[:,0]\n",
    "        all_post = CritPairs[:,7]\n",
    "        thisDiff = all_post - all_pre\n",
    "        \n",
    "        # This saves the output to a csv file\n",
    "        outFile = pd.DataFrame(thisDiff)\n",
    "        outFile.to_csv('./csvout/{}_{}.csv'.format(roi, hemi))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DG-effects'></a>\n",
    "#### Unpacking the main effect in DG\n",
    "The following quantifies the true effect at each similarity level, then the differences between peaks and trough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDict = {}\n",
    "\n",
    "us = []\n",
    "for i in range(shuffles):\n",
    "    samp = np.random.choice(36,36,True)\n",
    "    _samp = np.mean(thisDiff[samp], 0)\n",
    "    us.append(_samp)\n",
    "us = np.array(us)\n",
    "us = np.arctanh(us)\n",
    "for simlevel in range(8):\n",
    "    sim = us[:,simlevel]\n",
    "    LB = np.percentile(sim, 2.5)\n",
    "    U = np.mean(sim)\n",
    "    UB = np.percentile(sim, 97.5)\n",
    "    simDict[simlevel] = [LB, U, UB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for simlevel in range(8):\n",
    "    LB, U, UB = simDict[simlevel]\n",
    "    print('For similarity level {} -- M = {}, CI95 = [{} {}]'.format(simlevel + 1, \n",
    "                                                                     np.around(U, 4),\n",
    "                                                                     np.around(LB, 4), \n",
    "                                                                     np.around(UB, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pairing in [[3,4], [7,4]]:\n",
    "    sim1 = us[:,pairing[0]]\n",
    "    sim2 = us[:,pairing[1]]\n",
    "    diff = sim1-sim2\n",
    "    LB = np.percentile(diff, 2.5)\n",
    "    U = np.mean(diff)\n",
    "    UB = np.percentile(diff, 97.5)\n",
    "    print('For {} - {} contrast -- M = {}, CI95 = [{} {}]'.format(pairing[0]+1, pairing[1] + 1, \n",
    "                                                                  np.around(U, 17),\n",
    "                                                                  np.around(LB, 4), \n",
    "                                                                  np.around(UB, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nmph-model'></a>\n",
    "#### NMPH Model Fitting\n",
    "This fits the cubic model predicted by the NMPH based on all but a held out subject, then predicts values for the held out subject based on the fit parameters from the remainder of the sample. What follows is a bootstrap resampled estimate of the average correlation between model predictions and and actual observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to change which ROIs or hemispheres are analyzed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roiList = ['hipp', 'ca1', 'ca23', 'dg']\n",
    "roiList = ['PHC', 'perirhinal', 'EC']\n",
    "runTypes = ['both']\n",
    "resultDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rn, roi in enumerate(roiList[:]):\n",
    "    for hemi in runTypes:\n",
    "        print('Running {} hemisphere {}'.format(hemi, roi))\n",
    "        clear_output(wait=True)\n",
    "        RESULTS = []\n",
    "        AICS = []\n",
    "        \n",
    "        # read in the relevant data, subset to only the critical values\n",
    "        dat = pd.read_csv('./csvout/{}_{}.csv'.format(roi, hemi))\n",
    "        raw = np.array(dat.iloc[:,1:])\n",
    "        \n",
    "        # convert to pandas dataframe suitable for this analysis\n",
    "        melted = pd.melt(dat, id_vars='Unnamed: 0', var_name='simLevel', value_name='Change')\n",
    "        melted.rename(columns={'Unnamed: 0':'subID'}, inplace=True)\n",
    "        rvals = []\n",
    "        \n",
    "        for subject in range(raw.shape[0]):\n",
    "            # compute actual ~ predicted correlation for a given held out subject\n",
    "            corr = lv1outPoly(melted, subject, constrain=True, plot=True, order=3)\n",
    "            # Compile subjects in list\n",
    "            rvals.append(corr)\n",
    "        rvals = np.array(rvals)\n",
    "        RVALS = rvals if rn == 0 else np.vstack((RVALS, rvals))\n",
    "        print(RVALS.shape)\n",
    "        us = np.zeros((shuffles))\n",
    "        # Bootstrap resample a number of times, find 95% CI\n",
    "        for shuf in range(shuffles):\n",
    "            print('Running {} hemisphere {} -- {}/{}'.format(hemi, roi, shuf+1, shuffles))\n",
    "            clear_output(wait=True)\n",
    "            test = np.random.choice(rvals.shape[0], rvals.shape[0])\n",
    "            u = np.mean(rvals[test])\n",
    "            us[shuf] = u\n",
    "        # Fisher transform for statistical analysis\n",
    "        us = np.arctanh(us)\n",
    "        LB = np.percentile(us, 2.5)\n",
    "        U = np.mean(us)\n",
    "        UB = np.percentile(us, 97.5)\n",
    "        resultDict['{}_{}'.format(hemi, roi)] = [LB, U, UB]\n",
    "\n",
    "        plt.xticks(np.arange(8), np.arange(1,9))\n",
    "        plt.title('Predictions for Held-out Subjects')\n",
    "        plt.xlabel('Similarity Level')\n",
    "        plt.ylabel('Change in Representational Similarity')\n",
    "        #plt.savefig('figures/predictions_{}_{}.pdf'.format(hemi, roi), dpi=600)\n",
    "        plt.show()\n",
    "#np.save(\"UShapeR.npy\", RVALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nmph-model'></a>\n",
    "#### Print bootstrap resampled results\n",
    "The following cell will print the output of your chosen hemispheres and ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    for hemi in runTypes:\n",
    "        LB, U, UB = resultDict['{}_{}'.format(hemi, roi)]\n",
    "        print('For {} hemisphere {} -- M = {}, CI95 = [{} {}]'.format(hemi, roi, \n",
    "                                                                      np.around(U, 4),\n",
    "                                                                      np.around(LB, 4), \n",
    "                                                                      np.around(UB, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='noise-dist'></a>\n",
    "### Comparing to noise\n",
    "Now, we explore how the distribution of values would look if the A and B items in a pair were arbitrary, rather than based on visual similarity, to ensure that the true effect in a given ROI exceeds the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseDict = {}\n",
    "peaksDict = {}\n",
    "shuffles = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hemi in runTypes:\n",
    "    stem = stemdict[hemi]\n",
    "    for i, roi in enumerate(roiList[:]):\n",
    "        # This is every intercorrelation (16 x 16)\n",
    "        AllIms = np.load(allBase.format('surf', stem, roi))\n",
    "        us = np.zeros((shuffles))\n",
    "        valleys = np.zeros((shuffles, 5))\n",
    "        for shuf in range(shuffles):\n",
    "            print('Running {} hemisphere {} -- {}/{}'.format(hemi, roi, shuf+1, shuffles))\n",
    "            clear_output(wait=True)\n",
    "           \n",
    "            all_pre, all_post = shufPrepost(AllIms)  \n",
    "            thisDiff = all_post - all_pre\n",
    "            valleys[shuf, 0] = np.mean(thisDiff, 0)[4]\n",
    "            valleys[shuf, 1] = np.mean(thisDiff, 0)[5]\n",
    "            valleys[shuf, 2] = np.mean(thisDiff, 0)[7]\n",
    "            valleys[shuf, 3] = np.mean(thisDiff, 0)[4] - np.mean(thisDiff, 0)[3]\n",
    "            valleys[shuf, 4] = np.mean(thisDiff, 0)[7] - np.mean(thisDiff, 0)[4]\n",
    "            dat = pd.DataFrame(thisDiff)\n",
    "            dat['subID'] = np.arange(len(subList))\n",
    "            thisIter = pd.melt(dat, id_vars=['subID'], var_name='simLevel', value_name='Change')\n",
    "            rvals = []\n",
    "            for subject in range(len(subList)):\n",
    "                # compute actual ~ predicted correlation for a given held out subject\n",
    "                corr = lv1outPoly(thisIter, subject, constrain=True, order=3)\n",
    "                # Compile subjects in list\n",
    "                rvals.append(corr)\n",
    "            us[shuf] = np.mean(np.array(rvals))\n",
    "        print(np.mean(valleys, 0))\n",
    "        us = np.arctanh(us)\n",
    "        _, trueU, _ = resultDict['{}_{}'.format(hemi, roi)]\n",
    "        percentile = (us > trueU).sum() / shuffles\n",
    "        LB = np.percentile(us, 2.5)\n",
    "        U = np.mean(us)\n",
    "        UB = np.percentile(us, 97.5)\n",
    "        noiseDict['{}_{}'.format(hemi, roi)] = [LB, U, UB, percentile]\n",
    "        plt.hist(us,100)\n",
    "        plt.axvline(LB)\n",
    "        plt.axvline(UB)\n",
    "        #plt.savefig('figures/noise_{}_{}.pdf'.format(hemi, roi), dpi=600)\n",
    "        plt.show()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    for hemi in runTypes:\n",
    "        LB, U, UB, percentile = noiseDict['{}_{}'.format(hemi, roi)]\n",
    "        print('For {} hemisphere {} -- M = {}, CI95 = [{} {}], percentile = {}'.format(hemi, roi, \n",
    "                                                                                       np.around(U, 4),\n",
    "                                                                                       np.around(LB, 4), \n",
    "                                                                                       np.around(UB, 4),\n",
    "                                                                                       np.around(percentile, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='posthoc'></a>\n",
    "### Post-hoc analyses of DG curve\n",
    "This will plot the actual values, bootstrap resampled across subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ['sim level 5', 'sim level 6', 'sim level 7']\n",
    "for i, simlevel in enumerate([4,5,7]):\n",
    "    noise = valleys[:, i]\n",
    "    _, trueparam, _ = simDict[simlevel]\n",
    "    percentile = (noise < trueparam).sum() / shuffles if i < 2 else (noise > trueparam).sum() / shuffles\n",
    "    extremeNoise = np.around(np.amin(noise), 4) if i < 2 else np.around(np.amax(noise), 4)\n",
    "    print('{} --- extreme noise value: {}, true value: {}, percentile WRT noise: {}'.format(descriptions[i],\n",
    "                                                                                            extremeNoise,\n",
    "                                                                                            np.around(trueparam,3),\n",
    "                                                                                            percentile))\n",
    "descriptions = ['peak1-trough diff', 'trough-peak2 diff']\n",
    "for i, (truediff, noisediff) in enumerate(zip([0.12920655615343055, 0.15013934406029517], [3, 4])):\n",
    "    noise = valleys[:, noisediff]\n",
    "    percentile = (noise > truediff).sum() / shuffles\n",
    "    extremeNoise = np.around(np.amax(noise), 4)\n",
    "    print('{} --- extreme noise value: {}, true value: {}, percentile WRT noise: {}'.format(descriptions[i],\n",
    "                                                                                            extremeNoise,\n",
    "                                                                                            np.around(truediff,3),\n",
    "                                                                                            percentile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plotting'></a>\n",
    "### Plotting the function\n",
    "This will plot the actual values, bootstrap resampled across subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to change which ROIs or hemispheres are analyzed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roiList = ['hipp', 'ca1', 'ca23', 'dg']\n",
    "runTypes = ['both']\n",
    "plottingDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffles = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    for hemi in runTypes:\n",
    "        dat = np.array(pd.read_csv('./csvout/{}_{}.csv'.format(roi, hemi)))[:, 1:]\n",
    "        us = np.zeros((shuffles, 8))\n",
    "        for shuf in range(shuffles):\n",
    "            IX = np.random.choice(dat.shape[0], dat.shape[0])\n",
    "            test = dat[IX, :]\n",
    "            us[shuf] = (np.mean(test, 0))\n",
    "        LB = np.percentile(us, 2.5, 0)\n",
    "        U = np.mean(us, 0)\n",
    "        UB = np.percentile(us, 97.5, 0)\n",
    "        plottingDict['{}_{}'.format(hemi, roi)] = [LB, U, UB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, roi in enumerate(roiList):\n",
    "    r,g,b = colorDict[roi]\n",
    "    r, g, b = r/255, g/255, b/255\n",
    "    for hemi in runTypes:\n",
    "        LB, U, UB = plottingDict['{}_{}'.format(hemi, roi)]\n",
    "        fig, ax = plt.subplots(figsize=(6,8))\n",
    "        plt.axhline(0, zorder=-5, color=(0.5, 0.5, 0.5), lw=2, ls='--')\n",
    "        plt.fill_between(np.arange(8), LB, UB, color = (1, 1, 1, 0.7), zorder=-2, lw=0)\n",
    "        plt.fill_between(np.arange(8), LB, UB, color = (r, g, b, 0.1), zorder=-2, lw=0)\n",
    "        plt.plot(U, lw=4, c=[r,g,b,1])\n",
    "        plt.scatter(np.arange(8), U, s=150, lw=1.5, edgecolor=[[r,g,b,1]], facecolor=[[r,g,b,0.6]])\n",
    "        plt.yticks(np.arange(-0.20, 0.16, 0.05), ['-0.20', '-0.15', '-0.10', '-0.05', '0.00', '0.05', '0.10', '0.15'], \n",
    "                   fontsize=18, **{'fontname':'Arial Narrow'})\n",
    "        plt.xticks(np.arange(8), np.arange(1,9), fontsize=18, **{'fontname':'Arial Narrow'})\n",
    "        plt.ylabel('Representational Change', fontsize=24, **{'fontname':'Arial Narrow'})\n",
    "        plt.xlabel('Similarity Level', fontsize=24, **{'fontname':'Arial Narrow'})\n",
    "        #plt.savefig('figures/truedata_{}_{}.pdf'.format(hemi, roi), dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure for Paper\n",
    "\n",
    "This is the figure that ends up in the paper, and depicts the learning effect in the subfields and hippocampus as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roiList = ['hipp', 'ca1', 'ca23', 'dg']\n",
    "sig = ['','','','*']\n",
    "sigdict = dict(zip(roiList, sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(roiList), figsize=(int(len(roiList)*4), 8), sharey=True)\n",
    "for hemi in runTypes:\n",
    "    for i, (ax, roi) in enumerate(zip(axes, roiList)):\n",
    "        r,g,b = colorDict[roi]\n",
    "        r, g, b = r/255, g/255, b/255\n",
    "        LB, U, UB = plottingDict['{}_{}'.format(hemi, roi)]\n",
    "        ax.axhline(0, zorder=-5, color=(0.5, 0.5, 0.5), lw=2, ls='--')\n",
    "        ax.fill_between(np.arange(8), LB, UB, color = (1, 1, 1, 0.7), zorder=-2, lw=0)\n",
    "        ax.fill_between(np.arange(8), LB, UB, color = (r, g, b, 0.1), zorder=-2, lw=0)\n",
    "        ax.plot(U, lw=4, c=[r,g,b,1])\n",
    "        ax.scatter(np.arange(8), U, s=150, lw=1.5, edgecolor=[[r,g,b,1]], facecolor=[[r,g,b,0.6]])\n",
    "        ax.set_xticks(np.arange(8))\n",
    "        ax.set_xticklabels(np.arange(1,9), fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "        ax.set_xlabel('Similarity Level', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "        ax.text(0, -0.18, '{}{}'.format(nameDict[roi],sigdict[roi]), color=(r,g,b,1), fontsize=64, **{'fontname':'DIN Condensed'})\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Representational Change', fontsize=28, **{'fontname':'Arial Narrow'})\n",
    "            ax.set_yticks(np.arange(-0.20, 0.16, 0.05))\n",
    "            ax.set_yticklabels(['-0.20', '-0.15', '-0.10', '-0.05', '0.00', '0.05', '0.10', '0.15'],\n",
    "                               fontsize=20, **{'fontname':'Arial Narrow'})\n",
    "    plt.ylim(-0.20, 0.15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/manuscript_{}.pdf'.format(hemi), dpi=600)\n",
    "    #plt.savefig('figures/supplement_{}.pdf'.format(hemi), dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brains for ROI view\n",
    "\n",
    "This is the slice that shows the hippocampal subfield segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat = 'brains/sub10_t2int1.nii.gz'\n",
    "subStem = 'brains/{}.nii.gz'\n",
    "cmap = plt.get_cmap('gist_gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.3, 1.0)\n",
    "\n",
    "base = nib.load(anat).get_data()\n",
    "\n",
    "for i, roi in enumerate(roiList):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(12,4))\n",
    "    r,g,b = colorDict[roi]\n",
    "    r, g, b = r/255, g/255, b/255\n",
    "    mask = nib.load(subStem.format(roi)).get_data()\n",
    "    forNow = np.copy(base)\n",
    "    forNow[mask == 1] = np.nan\n",
    "    thisSlice = np.rot90(forNow[45:165, 131,85:120])\n",
    "    ax.matshow(thisSlice, cmap=new_cmap)\n",
    "    for Y in range(thisSlice.shape[0]):\n",
    "        for X in range(thisSlice.shape[1]):\n",
    "            if np.isnan(thisSlice[Y, X]):\n",
    "                rect = patches.Rectangle((X-0.6, Y-0.6),1.1,1.1,linewidth=0,edgecolor='r',facecolor=(r,g,b))\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('off')\n",
    "    plt.savefig('figures/brain_{}.pdf'.format(roi), dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
